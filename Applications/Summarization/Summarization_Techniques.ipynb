{"cells":[{"cell_type":"markdown","source":["# Summarization using LLMs\n","\n","#### Developed By: Manaranjan Pradhan\n","#### www.manaranjanp.com\n","\n","*This Jupyter notebook is confidential and proprietary to Manaranjan Pradhan. It is intended solely for authorized training purposes. Unauthorized distribution, sharing, or reproduction of this notebook or its contents is strictly prohibited. This material is for personal learning within the training program only and may not be used for commercial purposes or shared with others. Unauthorized use may result in disciplinary action or legal consequences. If you have received this notebook without authorization, please contact manaranjan@gmail.com immediately and delete all copies.*"],"metadata":{"id":"4n3jUgdK-TEH"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RRYSu48huSUW","outputId":"72027b6e-1290-44e2-b648-ec8dafd85e2d","executionInfo":{"status":"ok","timestamp":1739280969616,"user_tz":-330,"elapsed":16513,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.0/413.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip -q install langchain openai tiktoken langchain_openai"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J-KFB7J_u_3L","outputId":"44ed833c-beb4-4a60-ef58-7befcf5ba82e","executionInfo":{"status":"ok","timestamp":1739280976512,"user_tz":-330,"elapsed":6886,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Name: langchain\n","Version: 0.3.17\n","Summary: Building applications with LLMs through composability\n","Home-page: https://github.com/langchain-ai/langchain\n","Author: \n","Author-email: \n","License: MIT\n","Location: /usr/local/lib/python3.11/dist-packages\n","Requires: aiohttp, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n","Required-by: \n"]}],"source":["!pip show langchain"]},{"cell_type":"markdown","metadata":{"id":"wW6FD6FsT5Qf"},"source":["# Summarization"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNA4TsHpu6OM","outputId":"344722b4-bcc0-49fd-d9b4-d9332b32d33d","executionInfo":{"status":"ok","timestamp":1739280983395,"user_tz":-330,"elapsed":6882,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your OpenAI API key: ··········\n"]}],"source":["import os\n","from getpass import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"]},{"cell_type":"markdown","metadata":{"id":"HqwsGJDhvAQ5"},"source":["### Setting up Summarization Chain"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"lgesD0jrvDyG","executionInfo":{"status":"ok","timestamp":1739281002243,"user_tz":-330,"elapsed":3434,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["from langchain import PromptTemplate, LLMChain\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.chains.mapreduce import MapReduceChain\n","from langchain.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"fZd0iFxEI2yc","executionInfo":{"status":"ok","timestamp":1739281002621,"user_tz":-330,"elapsed":379,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["llm = ChatOpenAI(model_name='gpt-3.5-turbo',\n","             temperature=0.2,\n","             max_tokens = 256)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"prTL5Dg7a4xb","outputId":"f6b4bee4-ff6b-44d4-9215-8ee4951dfceb","executionInfo":{"status":"ok","timestamp":1739281005974,"user_tz":-330,"elapsed":412,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-02-11 13:36:45--  https://raw.githubusercontent.com/manaranjanp/ISBNLPv1/main/datasets/gpu_shortage\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6947 (6.8K) [text/plain]\n","Saving to: ‘gpu_shortage’\n","\n","gpu_shortage        100%[===================>]   6.78K  --.-KB/s    in 0s      \n","\n","2025-02-11 13:36:45 (69.9 MB/s) - ‘gpu_shortage’ saved [6947/6947]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/manaranjanp/ISBNLPv1/main/datasets/gpu_shortage"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-D3RrGXkbAJh","outputId":"5a846c9d-9269-46a2-8e6b-be7b1522e9aa","executionInfo":{"status":"ok","timestamp":1739281008050,"user_tz":-330,"elapsed":126,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["total 24\n","drwxr-xr-x 1 root root 4096 Feb 11 13:36 .\n","drwxr-xr-x 1 root root 4096 Feb 11 13:32 ..\n","drwxr-xr-x 4 root root 4096 Feb  7 14:19 .config\n","-rw-r--r-- 1 root root 6947 Feb 11 13:36 gpu_shortage\n","drwxr-xr-x 1 root root 4096 Feb  7 14:20 sample_data\n"]}],"source":["!ls -al"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"sCfCSX9sOv2A","executionInfo":{"status":"ok","timestamp":1739281009723,"user_tz":-330,"elapsed":12,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["# load the doc\n","with open('gpu_shortage') as f:\n","    gpu_shortage_essay = f.read()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GylknJF_SjOX","outputId":"9fe71073-c382-4829-fcb5-3b6aac9f79f0","executionInfo":{"status":"ok","timestamp":1724075180471,"user_tz":-330,"elapsed":4,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6751"]},"metadata":{},"execution_count":9}],"source":["len(gpu_shortage_essay)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"PcPzxvZZUTox","executionInfo":{"status":"ok","timestamp":1739281013767,"user_tz":-330,"elapsed":3,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n","texts = text_splitter.split_text(gpu_shortage_essay)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6KdLobvkLje","outputId":"94a61064-5eb2-402e-b2fa-d90f091ae9f6","executionInfo":{"status":"ok","timestamp":1739281015408,"user_tz":-330,"elapsed":14,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{},"execution_count":10}],"source":["len(texts)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"8L6ztJUJO2j8","executionInfo":{"status":"ok","timestamp":1739281016228,"user_tz":-330,"elapsed":7,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["from langchain.docstore.document import Document\n","\n","docs = [Document(page_content=t) for t in texts[:4]]"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WlSA_nIxdrSU","outputId":"16f7e8b3-9a42-497e-c560-56c25bedee05","executionInfo":{"status":"ok","timestamp":1739281017647,"user_tz":-330,"elapsed":23,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={}, page_content='As compute-hungry generative AI shows no signs of slowing down, which companies are getting access to Nvidia’s hard-to-come-by, ultra-expensive, high-performance computing H100 GPU for large language model (LLM) training is becoming the “top gossip” of Silicon Valley, according to Andrej Karpathy, former director of AI at Tesla and now at OpenAI.\\n\\nKarpathy’s comments come at a moment where issues related to GPU access are even being discussed in big tech annual reports: In Microsoft’s annual report released last week, the company emphasized to investors that GPUs are a “critical raw material for its fast-growing cloud business” and added language about GPUs to a “risk factor for outages that can arise if it can’t get the infrastructure it needs.”'),\n"," Document(metadata={}, page_content='Karpathy took to the social network X (formerly Twitter) to re-share a widely circulated blog post thought to be authored by a poster on Hacker News that speculates “the capacity of large scale H100 clusters at small and large cloud providers is running out,” and that H100 demand will continue its trend till the end of 2024, at a minimum.\\n\\nThe author guesses that OpenAI might want 50,000 H100s, while Inflection wants 22,000, Meta “maybe 25k,” while “big clouds might want 30k each (Azure, Google Cloud, AWS, plus Oracle). Lambda and CoreWeave and the other private clouds might want 100k total. Anthropic, Helsing, Mistral and Character might want 10k each, he wrote.'),\n"," Document(metadata={}, page_content='The author said that these estimates are “total ballparks and guessing, and some of that is double-counting both the cloud and the end customer who will rent from the cloud. But that gets to about 432k H100s. At approx $35K a piece, that’s about $15B worth of GPUs. That also excludes Chinese companies like ByteDance (TikTok), Baidu and Tencent who will want a lot of H800s. There are also financial companies each doing deployments starting with hundreds of A100s or H100s and going to thousands of A/H100s: names like Jane Street, JP Morgan, Two Sigma, Citadel.”\\n\\nDemand for GPUs is like ‘Game of Thrones,’ says one VC\\nThe closest analogy to the battle to get access to AI chips is the television hit ‘Game of Thrones,’ David Katz, partner at Radical Ventures, told VentureBeat recently. “There’s this insatiable appetite for compute that’s required in order to run these models and large models,” he said.'),\n"," Document(metadata={}, page_content='Last year, Radical invested in CentML, which optimizes machine learning (ML) models to work faster and lower compute costs. CentML’s offering, he said, creates “a little bit more efficiency” in the market. In addition, it demonstrates that complex, billion-plus-parameter models can also run on legacy hardware.\\n\\n“So you don’t need the same volume of GPUs, or you don’t need the A100s necessarily,” he said. “From that perspective, it is essentially increasing the capacity or the supply of chips in the market.”\\n\\nHowever, those efforts may be more effective for those working on AI inference, rather than training LLMs from scratch, according to Sid Sheth, CEO of d-Matrix, which is building a platform to save money on inference by doing more processing in the computer’s memory, rather than on a GPU.')]"]},"metadata":{},"execution_count":12}],"source":["docs"]},{"cell_type":"markdown","metadata":{"id":"IczFCIKaLCet"},"source":["##  3 types of CombineDocuments Chains\n","\n","[Taken from the LangChain Docs](https://langchain.readthedocs.io/en/latest/modules/indexes/combine_docs.html)"]},{"cell_type":"markdown","metadata":{"id":"a6kbcja8O7-Q"},"source":["## Summarize Simple with map_reduce"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"4jVWsy4cXibp","executionInfo":{"status":"ok","timestamp":1739281025427,"user_tz":-330,"elapsed":9,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["prompt_template = \"\"\"Write a concise bullet point summary of the following:\n","\n","{text}\n","\n","CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n","\n","BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template,\n","                        input_variables=[\"text\"])"]},{"cell_type":"markdown","metadata":{"id":"9o7FgrlMhuW2"},"source":["\n","\n","### Stuffing\n","Stuffing is the simplest method, whereby you simply stuff all the related data into the prompt as context to pass to the language model. This is implemented in LangChain as the StuffDocumentsChain.\n","\n","**Pros:** Only makes a single call to the LLM. When generating text, the LLM has access to all the data at once.\n","\n","**Cons:** Most LLMs have a context length, and for large documents (or many documents) this will not work as it will result in a prompt larger than the context length.\n","\n","The main downside of this method is that **it only works one smaller pieces of data.**  Once you are working with many pieces of data, this approach is no longer feasible. The next two approaches are designed to help deal with that.\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"VHOa9DYTJUkq","executionInfo":{"status":"ok","timestamp":1739281045043,"user_tz":-330,"elapsed":40,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["import textwrap"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"r8bUzeg126Up","executionInfo":{"status":"ok","timestamp":1739281045625,"user_tz":-330,"elapsed":20,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["from langchain.chains.summarize import load_summarize_chain"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"nrtSb8VphVG-","executionInfo":{"status":"ok","timestamp":1739281047750,"user_tz":-330,"elapsed":1459,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["chain = load_summarize_chain(llm,\n","                             chain_type=\"stuff\",\n","                             prompt=BULLET_POINT_PROMPT)\n","\n","output_summary = chain.invoke(docs)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"eYOA5JQqJycL","executionInfo":{"status":"ok","timestamp":1739281047760,"user_tz":-330,"elapsed":2,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["from pprint import pprint"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AAur_y56JpPF","outputId":"ddd0a916-0131-49aa-8c69-a7121dff4720","executionInfo":{"status":"ok","timestamp":1739281048022,"user_tz":-330,"elapsed":59,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_documents': [Document(metadata={}, page_content='As compute-hungry generative AI shows no signs of slowing down, which companies are getting access to Nvidia’s hard-to-come-by, ultra-expensive, high-performance computing H100 GPU for large language model (LLM) training is becoming the “top gossip” of Silicon Valley, according to Andrej Karpathy, former director of AI at Tesla and now at OpenAI.\\n\\nKarpathy’s comments come at a moment where issues related to GPU access are even being discussed in big tech annual reports: In Microsoft’s annual report released last week, the company emphasized to investors that GPUs are a “critical raw material for its fast-growing cloud business” and added language about GPUs to a “risk factor for outages that can arise if it can’t get the infrastructure it needs.”'),\n","                     Document(metadata={}, page_content='Karpathy took to the social network X (formerly Twitter) to re-share a widely circulated blog post thought to be authored by a poster on Hacker News that speculates “the capacity of large scale H100 clusters at small and large cloud providers is running out,” and that H100 demand will continue its trend till the end of 2024, at a minimum.\\n\\nThe author guesses that OpenAI might want 50,000 H100s, while Inflection wants 22,000, Meta “maybe 25k,” while “big clouds might want 30k each (Azure, Google Cloud, AWS, plus Oracle). Lambda and CoreWeave and the other private clouds might want 100k total. Anthropic, Helsing, Mistral and Character might want 10k each, he wrote.'),\n","                     Document(metadata={}, page_content='The author said that these estimates are “total ballparks and guessing, and some of that is double-counting both the cloud and the end customer who will rent from the cloud. But that gets to about 432k H100s. At approx $35K a piece, that’s about $15B worth of GPUs. That also excludes Chinese companies like ByteDance (TikTok), Baidu and Tencent who will want a lot of H800s. There are also financial companies each doing deployments starting with hundreds of A100s or H100s and going to thousands of A/H100s: names like Jane Street, JP Morgan, Two Sigma, Citadel.”\\n\\nDemand for GPUs is like ‘Game of Thrones,’ says one VC\\nThe closest analogy to the battle to get access to AI chips is the television hit ‘Game of Thrones,’ David Katz, partner at Radical Ventures, told VentureBeat recently. “There’s this insatiable appetite for compute that’s required in order to run these models and large models,” he said.'),\n","                     Document(metadata={}, page_content='Last year, Radical invested in CentML, which optimizes machine learning (ML) models to work faster and lower compute costs. CentML’s offering, he said, creates “a little bit more efficiency” in the market. In addition, it demonstrates that complex, billion-plus-parameter models can also run on legacy hardware.\\n\\n“So you don’t need the same volume of GPUs, or you don’t need the A100s necessarily,” he said. “From that perspective, it is essentially increasing the capacity or the supply of chips in the market.”\\n\\nHowever, those efforts may be more effective for those working on AI inference, rather than training LLMs from scratch, according to Sid Sheth, CEO of d-Matrix, which is building a platform to save money on inference by doing more processing in the computer’s memory, rather than on a GPU.')],\n"," 'output_text': \"- Demand for Nvidia's H100 GPU for large language model \"\n","                'training is high in Silicon Valley\\n'\n","                '- Issues related to GPU access are being discussed in big '\n","                'tech annual reports\\n'\n","                '- Speculation suggests high demand for H100 GPUs from '\n","                'companies like OpenAI, Meta, and cloud providers\\n'\n","                '- Estimated demand for H100 GPUs is around 432k, totaling '\n","                'about $15B\\n'\n","                \"- VC compares demand for GPUs to 'Game of Thrones' battle for \"\n","                'access\\n'\n","                '- CentML offers optimization for ML models to run on legacy '\n","                'hardware, increasing chip supply\\n'\n","                '- Efforts to increase efficiency in AI inference may be more '\n","                'effective than training LLMs from scratch'}\n"]}],"source":["pprint(output_summary)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82luYdZOJaop","outputId":"716f2e95-944a-43f3-d4a1-41f38e791454","executionInfo":{"status":"ok","timestamp":1739281051505,"user_tz":-330,"elapsed":17,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["- Demand for Nvidia's H100 GPU for large language model training is high in Silicon Valley\n","- Issues\n","related to GPU access are being discussed in big tech annual reports\n","- Speculation suggests high\n","demand for H100 GPUs from companies like OpenAI, Meta, and cloud providers\n","- Estimated demand for\n","H100 GPUs is around 432k, totaling about $15B\n","- VC compares demand for GPUs to 'Game of Thrones'\n","battle for access\n","- CentML offers optimization for ML models to run on legacy hardware, increasing\n","chip supply\n","- Efforts to increase efficiency in AI inference may be more effective than training\n","LLMs from scratch\n"]}],"source":["wrapped_text = textwrap.fill(output_summary['output_text'],\n","                             width=100,\n","                             break_long_words=False,\n","                             replace_whitespace=False)\n","print(wrapped_text)"]},{"cell_type":"markdown","metadata":{"id":"cJ_ig4J_PYbm"},"source":["### map_reduce summarization with custom prompt"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"bUFsaodtPkl4","executionInfo":{"status":"ok","timestamp":1739281064859,"user_tz":-330,"elapsed":7826,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["# with a custom prompt\n","prompt_template = \"\"\"Write a concise summary of the following:\n","\n","{text}\n","\n","CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n","\n","PROMPT = PromptTemplate(template=prompt_template,\n","                        input_variables=[\"text\"])\n","\n","## with intermediate steps\n","chain = load_summarize_chain(llm,\n","                             chain_type=\"map_reduce\",\n","                             return_intermediate_steps=True,\n","                             map_prompt=PROMPT,\n","                             combine_prompt=PROMPT)\n","\n","output_summary = chain.invoke({\"input_documents\": docs}, return_only_outputs=True)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X2XPK9CyKZf8","outputId":"50c40371-6b91-4209-cf21-67707300a0ec","executionInfo":{"status":"ok","timestamp":1739281064884,"user_tz":-330,"elapsed":21,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["- Generative AI requires high-performance computing resources, with Nvidia's H100 GPU being highly\n","sought after in Silicon Valley\n","- GPU availability is a top gossip topic among tech companies and is\n","emphasized in Microsoft's annual report for its cloud business\n","- Demand for H100 GPUs is predicted\n","to continue until at least the end of 2024, with estimates suggesting a need for 432k GPUs worth\n","$15B\n","- Chinese companies and financial firms are also deploying large numbers of GPUs for AI\n","applications\n","- Companies like CentML and d-Matrix are working on optimizing machine learning models\n","for faster performance and cost savings, particularly for AI inference.\n"]}],"source":["wrapped_text = textwrap.fill(output_summary['output_text'],\n","                             width=100,\n","                             break_long_words=False,\n","                             replace_whitespace=False)\n","print(wrapped_text)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UsHRIVoGKiSf","outputId":"3211100b-dff7-47ff-905b-2185b049b8c8","executionInfo":{"status":"ok","timestamp":1739281067407,"user_tz":-330,"elapsed":14,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["- Karpathy shared a blog post speculating about the capacity of large scale H100 clusters at cloud\n","providers\n","- The post predicts that H100 demand will continue until at least the end of 2024\n","- The\n","author estimates that OpenAI, Inflection, Meta, and big cloud providers will want a significant\n","number of H100s\n","- Private clouds like Lambda and CoreWeave may also have high demand for H100s\n"]}],"source":["wrapped_text = textwrap.fill(output_summary['intermediate_steps'][1],\n","                             width=100,\n","                             break_long_words=False,\n","                             replace_whitespace=False)\n","print(wrapped_text)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OvksjoTytSCb","outputId":"20eb63a2-3710-4888-f251-94d5e3d581af","executionInfo":{"status":"ok","timestamp":1739281068423,"user_tz":-330,"elapsed":16,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["- Estimates suggest there is a demand for approximately 432k H100 GPUs, totaling about $15B worth of\n","GPUs\n","- Chinese companies like ByteDance, Baidu, and Tencent are also expected to require a\n","significant number of H800 GPUs\n","- Financial companies such as Jane Street, JP Morgan, Two Sigma, and\n","Citadel are deploying hundreds to thousands of A/H100 GPUs\n","- The demand for GPUs for AI applications\n","is compared to the battle for power in 'Game of Thrones' by a venture capitalist\n"]}],"source":["wrapped_text = textwrap.fill(output_summary['intermediate_steps'][2],\n","                             width=100,\n","                             break_long_words=False,\n","                             replace_whitespace=False)\n","print(wrapped_text)"]},{"cell_type":"markdown","metadata":{"id":"caaAmomfPv9j"},"source":["### With the 'refine' CombineDocument Chain"]},{"cell_type":"markdown","metadata":{"id":"39P6zjy9lT5X"},"source":["## Refine\n","This method involves **an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document**, asking the LLM to refine the output based on the new document.\n","\n","**Pros:** Can pull in more relevant context, and may be less lossy than MapReduceDocumentsChain.\n","\n","**Cons:** Requires many more calls to the LLM than StuffDocumentsChain. The calls are also NOT independent, meaning they cannot be paralleled like MapReduceDocumentsChain. There is also some potential dependencies on the ordering of the documents."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"7Ji2D8Q0P2gh","executionInfo":{"status":"ok","timestamp":1739281077887,"user_tz":-330,"elapsed":6384,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[],"source":["chain = load_summarize_chain(llm, chain_type=\"refine\")\n","\n","output_summary = chain.invoke(docs)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OGJ46apzKm3P","outputId":"c5d33735-4ceb-45ff-dd2b-dbf2ca69e7b3","executionInfo":{"status":"ok","timestamp":1739281077918,"user_tz":-330,"elapsed":29,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["The demand for Nvidia's high-performance computing H100 GPU for large language model training is\n","increasing among companies, leading to competition and speculation in Silicon Valley. Issues related\n","to GPU access are being highlighted in big tech annual reports, with companies like Microsoft\n","emphasizing the importance of GPUs for their cloud business and the potential risks of\n","infrastructure shortages. Speculation on the capacity of large-scale H100 clusters at cloud\n","providers and the projected demand from various companies, including OpenAI, Inflection, Meta, and\n","others, suggests that the demand for H100 GPUs will continue to rise until at least the end of 2024.\n","Estimates suggest a potential need for 432k H100 GPUs, valued at approximately $15B, with additional\n","demand expected from Chinese companies like ByteDance, Baidu, and Tencent, as well as financial\n","institutions like Jane Street, JP Morgan, Two Sigma, and Citadel. The competition for access to AI\n","chips is likened to the battles in 'Game of Thrones,' reflecting the insatiable appetite for compute\n","required to run large models. Radical's investment in CentML, which optimizes machine learning\n","models to work faster and lower compute costs, may increase the supply of chips in the market,\n","particularly for AI inference tasks. However, efforts like those from d-Matrix\n"]}],"source":["wrapped_text = textwrap.fill(output_summary['output_text'], width=100)\n","print(wrapped_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VhFoMHoGKloi"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}