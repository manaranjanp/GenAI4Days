{"cells":[{"cell_type":"markdown","source":["# Summarization using LLMs\n","\n","#### Developed By: Manaranjan Pradhan\n","#### www.manaranjanp.com\n","\n","*This Jupyter notebook is confidential and proprietary to Manaranjan Pradhan. It is intended solely for authorized training purposes. Unauthorized distribution, sharing, or reproduction of this notebook or its contents is strictly prohibited. This material is for personal learning within the training program only and may not be used for commercial purposes or shared with others. Unauthorized use may result in disciplinary action or legal consequences. If you have received this notebook without authorization, please contact manaranjan@gmail.com immediately and delete all copies.*"],"metadata":{"id":"4n3jUgdK-TEH"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RRYSu48huSUW","outputId":"30ad6d49-2f35-4ff0-867b-ac83df8d5612","executionInfo":{"status":"ok","timestamp":1741070850267,"user_tz":-330,"elapsed":8410,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip -q install langchain openai tiktoken langchain_openai langchain_groq"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J-KFB7J_u_3L","outputId":"5c0749e4-35fc-46be-fe97-cfd0d02f342a","executionInfo":{"status":"ok","timestamp":1741070852795,"user_tz":-330,"elapsed":2523,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Name: langchain\n","Version: 0.3.19\n","Summary: Building applications with LLMs through composability\n","Home-page: \n","Author: \n","Author-email: \n","License: MIT\n","Location: /usr/local/lib/python3.11/dist-packages\n","Requires: aiohttp, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n","Required-by: \n"]}],"source":["!pip show langchain"]},{"cell_type":"markdown","metadata":{"id":"wW6FD6FsT5Qf"},"source":["# Summarization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNA4TsHpu6OM","outputId":"5767d7cd-b067-443b-adc6-c3bf5a1628bc","executionInfo":{"status":"ok","timestamp":1741070859477,"user_tz":-330,"elapsed":6671,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your Groq API key: ··········\n"]}],"source":["import os\n","from getpass import getpass\n","\n","#os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n","os.environ[\"GROQ_API_KEY\"] = getpass(\"Enter your Groq API key: \")"]},{"cell_type":"markdown","metadata":{"id":"HqwsGJDhvAQ5"},"source":["### Setting up Summarization Chain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lgesD0jrvDyG"},"outputs":[],"source":["from langchain import PromptTemplate, LLMChain\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.chains.mapreduce import MapReduceChain\n","from langchain.prompts import PromptTemplate\n","from langchain_openai import ChatOpenAI\n","from langchain_groq import ChatGroq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fZd0iFxEI2yc"},"outputs":[],"source":["#llm = ChatOpenAI(model_name='gpt-3.5-turbo',\n","#             temperature=0.2,\n","#             max_tokens = 256)\n","\n","llm = ChatGroq(\n","    model=\"llama-3.3-70b-versatile\",\n","    temperature=0,\n","    max_tokens=256,\n","    max_retries=2,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"prTL5Dg7a4xb","outputId":"9a8abd75-e622-43c6-9def-0d913ea9f9e4","executionInfo":{"status":"ok","timestamp":1741070880103,"user_tz":-330,"elapsed":214,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-03-04 06:47:59--  https://raw.githubusercontent.com/manaranjanp/ISBNLPv1/main/datasets/gpu_shortage\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6947 (6.8K) [text/plain]\n","Saving to: ‘gpu_shortage’\n","\n","\rgpu_shortage          0%[                    ]       0  --.-KB/s               \rgpu_shortage        100%[===================>]   6.78K  --.-KB/s    in 0s      \n","\n","2025-03-04 06:47:59 (67.6 MB/s) - ‘gpu_shortage’ saved [6947/6947]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/manaranjanp/ISBNLPv1/main/datasets/gpu_shortage"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-D3RrGXkbAJh","outputId":"8436ca17-dfe9-4cdf-a5db-7d6efc471bc7","executionInfo":{"status":"ok","timestamp":1741070880221,"user_tz":-330,"elapsed":103,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["total 32\n","drwxr-xr-x 1 root root 4096 Mar  4 06:47  .\n","drwxr-xr-x 1 root root 4096 Mar  4 06:25  ..\n","drwxr-xr-x 4 root root 4096 Feb 28 14:19  .config\n","-rw-r--r-- 1 root root 6947 Mar  4 06:31 'gpu shortage'\n","-rw-r--r-- 1 root root 6947 Mar  4 06:47  gpu_shortage\n","drwxr-xr-x 1 root root 4096 Feb 28 14:20  sample_data\n"]}],"source":["!ls -al"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sCfCSX9sOv2A"},"outputs":[],"source":["# load the doc\n","with open('gpu_shortage') as f:\n","    gpu_shortage_essay = f.read()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GylknJF_SjOX","outputId":"666411e2-c6dc-4357-81e4-458733714a8d","executionInfo":{"status":"ok","timestamp":1741070880269,"user_tz":-330,"elapsed":13,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6751"]},"metadata":{},"execution_count":10}],"source":["len(gpu_shortage_essay)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PcPzxvZZUTox"},"outputs":[],"source":["text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n","texts = text_splitter.split_text(gpu_shortage_essay)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6KdLobvkLje","outputId":"eaf90e23-ec02-4173-8776-a97b41bdb0e0","executionInfo":{"status":"ok","timestamp":1741070880471,"user_tz":-330,"elapsed":2,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{},"execution_count":12}],"source":["len(texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8L6ztJUJO2j8"},"outputs":[],"source":["from langchain.docstore.document import Document\n","\n","docs = [Document(page_content=t) for t in texts[:4]]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WlSA_nIxdrSU","outputId":"74918b26-cf3a-4a3d-faa3-b757b1dadc0b","executionInfo":{"status":"ok","timestamp":1741070880491,"user_tz":-330,"elapsed":19,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(metadata={}, page_content='As compute-hungry generative AI shows no signs of slowing down, which companies are getting access to Nvidia’s hard-to-come-by, ultra-expensive, high-performance computing H100 GPU for large language model (LLM) training is becoming the “top gossip” of Silicon Valley, according to Andrej Karpathy, former director of AI at Tesla and now at OpenAI.\\n\\nKarpathy’s comments come at a moment where issues related to GPU access are even being discussed in big tech annual reports: In Microsoft’s annual report released last week, the company emphasized to investors that GPUs are a “critical raw material for its fast-growing cloud business” and added language about GPUs to a “risk factor for outages that can arise if it can’t get the infrastructure it needs.”'),\n"," Document(metadata={}, page_content='Karpathy took to the social network X (formerly Twitter) to re-share a widely circulated blog post thought to be authored by a poster on Hacker News that speculates “the capacity of large scale H100 clusters at small and large cloud providers is running out,” and that H100 demand will continue its trend till the end of 2024, at a minimum.\\n\\nThe author guesses that OpenAI might want 50,000 H100s, while Inflection wants 22,000, Meta “maybe 25k,” while “big clouds might want 30k each (Azure, Google Cloud, AWS, plus Oracle). Lambda and CoreWeave and the other private clouds might want 100k total. Anthropic, Helsing, Mistral and Character might want 10k each, he wrote.'),\n"," Document(metadata={}, page_content='The author said that these estimates are “total ballparks and guessing, and some of that is double-counting both the cloud and the end customer who will rent from the cloud. But that gets to about 432k H100s. At approx $35K a piece, that’s about $15B worth of GPUs. That also excludes Chinese companies like ByteDance (TikTok), Baidu and Tencent who will want a lot of H800s. There are also financial companies each doing deployments starting with hundreds of A100s or H100s and going to thousands of A/H100s: names like Jane Street, JP Morgan, Two Sigma, Citadel.”\\n\\nDemand for GPUs is like ‘Game of Thrones,’ says one VC\\nThe closest analogy to the battle to get access to AI chips is the television hit ‘Game of Thrones,’ David Katz, partner at Radical Ventures, told VentureBeat recently. “There’s this insatiable appetite for compute that’s required in order to run these models and large models,” he said.'),\n"," Document(metadata={}, page_content='Last year, Radical invested in CentML, which optimizes machine learning (ML) models to work faster and lower compute costs. CentML’s offering, he said, creates “a little bit more efficiency” in the market. In addition, it demonstrates that complex, billion-plus-parameter models can also run on legacy hardware.\\n\\n“So you don’t need the same volume of GPUs, or you don’t need the A100s necessarily,” he said. “From that perspective, it is essentially increasing the capacity or the supply of chips in the market.”\\n\\nHowever, those efforts may be more effective for those working on AI inference, rather than training LLMs from scratch, according to Sid Sheth, CEO of d-Matrix, which is building a platform to save money on inference by doing more processing in the computer’s memory, rather than on a GPU.')]"]},"metadata":{},"execution_count":14}],"source":["docs"]},{"cell_type":"markdown","metadata":{"id":"IczFCIKaLCet"},"source":["##  3 types of CombineDocuments Chains\n","\n","[Taken from the LangChain Docs](https://langchain.readthedocs.io/en/latest/modules/indexes/combine_docs.html)"]},{"cell_type":"markdown","metadata":{"id":"a6kbcja8O7-Q"},"source":["## Summarize Simple with map_reduce"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4jVWsy4cXibp"},"outputs":[],"source":["prompt_template = \"\"\"Write a concise bullet point summary of the following:\n","\n","{text}\n","\n","CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n","\n","BULLET_POINT_PROMPT = PromptTemplate(template=prompt_template,\n","                        input_variables=[\"text\"])"]},{"cell_type":"markdown","metadata":{"id":"9o7FgrlMhuW2"},"source":["\n","\n","### Stuffing\n","Stuffing is the simplest method, whereby you simply stuff all the related data into the prompt as context to pass to the language model. This is implemented in LangChain as the StuffDocumentsChain.\n","\n","**Pros:** Only makes a single call to the LLM. When generating text, the LLM has access to all the data at once.\n","\n","**Cons:** Most LLMs have a context length, and for large documents (or many documents) this will not work as it will result in a prompt larger than the context length.\n","\n","The main downside of this method is that **it only works one smaller pieces of data.**  Once you are working with many pieces of data, this approach is no longer feasible. The next two approaches are designed to help deal with that.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHOa9DYTJUkq"},"outputs":[],"source":["import textwrap"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8bUzeg126Up"},"outputs":[],"source":["from langchain.chains.summarize import load_summarize_chain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nrtSb8VphVG-"},"outputs":[],"source":["chain = load_summarize_chain(llm,\n","                             chain_type=\"stuff\",\n","                             prompt=BULLET_POINT_PROMPT)\n","\n","output_summary = chain.invoke(docs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eYOA5JQqJycL"},"outputs":[],"source":["from pprint import pprint"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AAur_y56JpPF","outputId":"f1acfe74-825a-491b-957d-edfab9535be4","executionInfo":{"status":"ok","timestamp":1741070889878,"user_tz":-330,"elapsed":24,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_documents': [Document(metadata={}, page_content='As compute-hungry generative AI shows no signs of slowing down, which companies are getting access to Nvidia’s hard-to-come-by, ultra-expensive, high-performance computing H100 GPU for large language model (LLM) training is becoming the “top gossip” of Silicon Valley, according to Andrej Karpathy, former director of AI at Tesla and now at OpenAI.\\n\\nKarpathy’s comments come at a moment where issues related to GPU access are even being discussed in big tech annual reports: In Microsoft’s annual report released last week, the company emphasized to investors that GPUs are a “critical raw material for its fast-growing cloud business” and added language about GPUs to a “risk factor for outages that can arise if it can’t get the infrastructure it needs.”'),\n","                     Document(metadata={}, page_content='Karpathy took to the social network X (formerly Twitter) to re-share a widely circulated blog post thought to be authored by a poster on Hacker News that speculates “the capacity of large scale H100 clusters at small and large cloud providers is running out,” and that H100 demand will continue its trend till the end of 2024, at a minimum.\\n\\nThe author guesses that OpenAI might want 50,000 H100s, while Inflection wants 22,000, Meta “maybe 25k,” while “big clouds might want 30k each (Azure, Google Cloud, AWS, plus Oracle). Lambda and CoreWeave and the other private clouds might want 100k total. Anthropic, Helsing, Mistral and Character might want 10k each, he wrote.'),\n","                     Document(metadata={}, page_content='The author said that these estimates are “total ballparks and guessing, and some of that is double-counting both the cloud and the end customer who will rent from the cloud. But that gets to about 432k H100s. At approx $35K a piece, that’s about $15B worth of GPUs. That also excludes Chinese companies like ByteDance (TikTok), Baidu and Tencent who will want a lot of H800s. There are also financial companies each doing deployments starting with hundreds of A100s or H100s and going to thousands of A/H100s: names like Jane Street, JP Morgan, Two Sigma, Citadel.”\\n\\nDemand for GPUs is like ‘Game of Thrones,’ says one VC\\nThe closest analogy to the battle to get access to AI chips is the television hit ‘Game of Thrones,’ David Katz, partner at Radical Ventures, told VentureBeat recently. “There’s this insatiable appetite for compute that’s required in order to run these models and large models,” he said.'),\n","                     Document(metadata={}, page_content='Last year, Radical invested in CentML, which optimizes machine learning (ML) models to work faster and lower compute costs. CentML’s offering, he said, creates “a little bit more efficiency” in the market. In addition, it demonstrates that complex, billion-plus-parameter models can also run on legacy hardware.\\n\\n“So you don’t need the same volume of GPUs, or you don’t need the A100s necessarily,” he said. “From that perspective, it is essentially increasing the capacity or the supply of chips in the market.”\\n\\nHowever, those efforts may be more effective for those working on AI inference, rather than training LLMs from scratch, according to Sid Sheth, CEO of d-Matrix, which is building a platform to save money on inference by doing more processing in the computer’s memory, rather than on a GPU.')],\n"," 'output_text': 'Here is a concise summary of the article in bullet points:\\n'\n","                '\\n'\n","                \"* Demand for Nvidia's high-performance H100 GPU is extremely \"\n","                'high, particularly for large language model (LLM) training, '\n","                'with companies like OpenAI, Meta, and Microsoft competing for '\n","                'access.\\n'\n","                '* The shortage of H100 GPUs is becoming a major issue, with '\n","                'estimated demand totaling around 432,000 units, worth '\n","                'approximately $15 billion.\\n'\n","                '* Companies are looking for ways to optimize their use of '\n","                'GPUs, with some investing in technologies that can reduce '\n","                'compute costs and increase efficiency.\\n'\n","                '* The battle for access to AI chips is likened to \"Game of '\n","                'Thrones,\" with companies fighting for limited resources to '\n","                'power their AI models.\\n'\n","                '* Alternative solutions, such as optimizing ML models to work '\n","                'on legacy hardware, may help increase supply, but may be more '\n","                'effective for AI inference rather than LLM training.'}\n"]}],"source":["pprint(output_summary)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82luYdZOJaop","outputId":"0ac329db-c973-47ad-9c37-507d5eaa30f8","executionInfo":{"status":"ok","timestamp":1741070892300,"user_tz":-330,"elapsed":4,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Here is a concise summary of the article in bullet points:\n","\n","* Demand for Nvidia's high-performance\n","H100 GPU is extremely high, particularly for large language model (LLM) training, with companies\n","like OpenAI, Meta, and Microsoft competing for access.\n","* The shortage of H100 GPUs is becoming a\n","major issue, with estimated demand totaling around 432,000 units, worth approximately $15 billion.\n","*\n","Companies are looking for ways to optimize their use of GPUs, with some investing in technologies\n","that can reduce compute costs and increase efficiency.\n","* The battle for access to AI chips is\n","likened to \"Game of Thrones,\" with companies fighting for limited resources to power their AI\n","models.\n","* Alternative solutions, such as optimizing ML models to work on legacy hardware, may help\n","increase supply, but may be more effective for AI inference rather than LLM training.\n"]}],"source":["wrapped_text = textwrap.fill(output_summary['output_text'],\n","                             width=100,\n","                             break_long_words=False,\n","                             replace_whitespace=False)\n","print(wrapped_text)"]},{"cell_type":"markdown","metadata":{"id":"cJ_ig4J_PYbm"},"source":["### map_reduce summarization with custom prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bUFsaodtPkl4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741071270945,"user_tz":-330,"elapsed":4860,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}},"outputId":"01725ea1-210e-42b7-fb57-8587374a6c97"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mWrite a concise summary of the following chunk froma document:\n","\n","As compute-hungry generative AI shows no signs of slowing down, which companies are getting access to Nvidia’s hard-to-come-by, ultra-expensive, high-performance computing H100 GPU for large language model (LLM) training is becoming the “top gossip” of Silicon Valley, according to Andrej Karpathy, former director of AI at Tesla and now at OpenAI.\n","\n","Karpathy’s comments come at a moment where issues related to GPU access are even being discussed in big tech annual reports: In Microsoft’s annual report released last week, the company emphasized to investors that GPUs are a “critical raw material for its fast-growing cloud business” and added language about GPUs to a “risk factor for outages that can arise if it can’t get the infrastructure it needs.”\n","\n","CONSCISE SUMMARY IN BULLET POINTS:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mWrite a concise summary of the following chunk froma document:\n","\n","Karpathy took to the social network X (formerly Twitter) to re-share a widely circulated blog post thought to be authored by a poster on Hacker News that speculates “the capacity of large scale H100 clusters at small and large cloud providers is running out,” and that H100 demand will continue its trend till the end of 2024, at a minimum.\n","\n","The author guesses that OpenAI might want 50,000 H100s, while Inflection wants 22,000, Meta “maybe 25k,” while “big clouds might want 30k each (Azure, Google Cloud, AWS, plus Oracle). Lambda and CoreWeave and the other private clouds might want 100k total. Anthropic, Helsing, Mistral and Character might want 10k each, he wrote.\n","\n","CONSCISE SUMMARY IN BULLET POINTS:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mWrite a concise summary of the following chunk froma document:\n","\n","The author said that these estimates are “total ballparks and guessing, and some of that is double-counting both the cloud and the end customer who will rent from the cloud. But that gets to about 432k H100s. At approx $35K a piece, that’s about $15B worth of GPUs. That also excludes Chinese companies like ByteDance (TikTok), Baidu and Tencent who will want a lot of H800s. There are also financial companies each doing deployments starting with hundreds of A100s or H100s and going to thousands of A/H100s: names like Jane Street, JP Morgan, Two Sigma, Citadel.”\n","\n","Demand for GPUs is like ‘Game of Thrones,’ says one VC\n","The closest analogy to the battle to get access to AI chips is the television hit ‘Game of Thrones,’ David Katz, partner at Radical Ventures, told VentureBeat recently. “There’s this insatiable appetite for compute that’s required in order to run these models and large models,” he said.\n","\n","CONSCISE SUMMARY IN BULLET POINTS:\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mWrite a concise summary of the following chunk froma document:\n","\n","Last year, Radical invested in CentML, which optimizes machine learning (ML) models to work faster and lower compute costs. CentML’s offering, he said, creates “a little bit more efficiency” in the market. In addition, it demonstrates that complex, billion-plus-parameter models can also run on legacy hardware.\n","\n","“So you don’t need the same volume of GPUs, or you don’t need the A100s necessarily,” he said. “From that perspective, it is essentially increasing the capacity or the supply of chips in the market.”\n","\n","However, those efforts may be more effective for those working on AI inference, rather than training LLMs from scratch, according to Sid Sheth, CEO of d-Matrix, which is building a platform to save money on inference by doing more processing in the computer’s memory, rather than on a GPU.\n","\n","CONSCISE SUMMARY IN BULLET POINTS:\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mSummarize the summaries below. Create final summary by consolidating the summaries below in bullet point.\n","\n","Here is a concise summary in bullet points:\n","\n","* Access to Nvidia's high-performance H100 GPU is highly sought after for large language model training.\n","* The limited availability of these GPUs is a topic of interest in Silicon Valley.\n","* Major tech companies, such as Microsoft, consider GPUs a critical resource and a potential risk factor for their business.\n","\n","Here is a concise summary in bullet points:\n","\n","* A blog post speculates that large-scale H100 clusters at cloud providers are running out of capacity.\n","* Demand for H100s is expected to continue until at least the end of 2024.\n","* Estimated H100 demands:\n","  + OpenAI: 50,000\n","  + Inflection: 22,000\n","  + Meta: 25,000\n","  + Big clouds (Azure, Google Cloud, AWS, Oracle): 30,000 each\n","  + Private clouds (Lambda, CoreWeave, etc.): 100,000 total\n","  + Other companies (Anthropic, Helsing, Mistral, Character): 10,000 each\n","\n","Here is a concise summary of the text in bullet points:\n","\n","* Estimated demand for H100 GPUs: approximately 432,000 units, worth around $15 billion\n","* This estimate excludes Chinese companies like ByteDance, Baidu, and Tencent\n","* Financial companies like Jane Street, JP Morgan, and Citadel are also deploying hundreds to thousands of A100/H100 GPUs\n","* Demand for AI chips is extremely high, likened to the battle for power in \"Game of Thrones\" due to the insatiable need for compute power to run large AI models.\n","\n","Here is a concise summary in bullet points:\n","\n","* Radical invested in CentML, which optimizes machine learning models for faster performance and lower compute costs.\n","* CentML's technology allows complex models to run on legacy hardware, reducing the need for high-end GPUs.\n","* This can increase the supply of available chips in the market, but may be more beneficial for AI inference rather than training large language models.\n","\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]}],"source":["# with a custom prompt\n","map_prompt_template = \"\"\"Write a concise summary of the following chunk froma document:\n","\n","{text}\n","\n","CONSCISE SUMMARY IN BULLET POINTS:\"\"\"\n","\n","reduce_prompt_template = \"\"\"Summarize the summaries below. Create final summary by consolidating the summaries below in bullet point.\n","\n","{text}\n","\"\"\"\n","\n","MAP_PROMPT = PromptTemplate(template=map_prompt_template,\n","                        input_variables=[\"text\"])\n","\n","REDUCE_PROMPT = PromptTemplate(template=reduce_prompt_template,\n","                               input_variables=[\"text\"])\n","\n","## with intermediate steps\n","chain = load_summarize_chain(llm,\n","                             chain_type=\"map_reduce\",\n","                             return_intermediate_steps=True,\n","                             map_prompt=MAP_PROMPT,\n","                             combine_prompt=REDUCE_PROMPT,\n","                             verbose = True)\n","\n","output_summary = chain.invoke({\"input_documents\": docs}, return_only_outputs=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X2XPK9CyKZf8","outputId":"05b571f7-a669-4533-cf2e-1690396cc3bc","executionInfo":{"status":"ok","timestamp":1741071138270,"user_tz":-330,"elapsed":54,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Here is a final summary consolidating the information in bullet points:\n","\n","* The demand for Nvidia's\n","H100 GPUs is extremely high, driven by large language model training, with an estimated demand of\n","approximately 432,000 units, worth around $15 billion.\n","* Major tech companies, such as Microsoft,\n","OpenAI, Meta, and others, are seeking large quantities of H100 GPUs, with some estimates including:\n","+ OpenAI: 50,000\n","  + Inflection: 22,000\n","  + Meta: 25,000\n","  + Big clouds: 30,000 each\n","  + Private\n","clouds: 100,000 total\n","* The limited availability of H100 GPUs is a concern for these companies, with\n","some speculating that large-scale H100 clusters at cloud providers are running out of capacity.\n","*\n","Financial companies and others are also deploying hundreds to thousands of A100/H100 GPUs, further\n","driving up demand.\n","* To address the shortage, companies like Radical are investing in technologies\n","like CentML, which optimizes machine learning models to run on legacy hardware, potentially\n","increasing the supply of available chips.\n","* The high demand for AI chips is likened to a battle for\n","power, with companies competing for access to these critical resources to support their AI\n"]}],"source":["wrapped_text = textwrap.fill(output_summary['output_text'],\n","                             width=100,\n","                             break_long_words=False,\n","                             replace_whitespace=False)\n","print(wrapped_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UsHRIVoGKiSf","outputId":"07a59906-2a98-4944-9612-ca1d21300819","executionInfo":{"status":"ok","timestamp":1741071249943,"user_tz":-330,"elapsed":15,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Here is a concise summary in bullet points:\n","\n","* A blog post speculates that large-scale H100 clusters\n","at cloud providers are running out of capacity.\n","* Demand for H100s is expected to continue until at\n","least the end of 2024.\n","* Estimated H100 demands:\n","  + OpenAI: 50,000\n","  + Inflection: 22,000\n","  + Meta:\n","25,000\n","  + Big clouds (Azure, Google Cloud, AWS, Oracle): 30,000 each\n","  + Private clouds (Lambda,\n","CoreWeave, etc.): 100,000 total\n","  + Anthropic, Helsing, Mistral, and Character: 10,000 each\n"]}],"source":["wrapped_text = textwrap.fill(output_summary['intermediate_steps'][1],\n","                             width=100,\n","                             break_long_words=False,\n","                             replace_whitespace=False)\n","print(wrapped_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OvksjoTytSCb","outputId":"d02ddf4e-c535-4f6b-a5b1-bc16c7ddb4d0","executionInfo":{"status":"ok","timestamp":1741071138284,"user_tz":-330,"elapsed":10,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Here is a concise summary in bullet points:\n","\n","* Estimated demand for H100 GPUs: approximately 432,000\n","units, worth around $15 billion\n","* This estimate excludes Chinese companies like ByteDance, Baidu,\n","and Tencent\n","* Financial companies like Jane Street, JP Morgan, and Citadel are also deploying\n","hundreds to thousands of A100/H100 GPUs\n","* Demand for AI chips is extremely high, likened to the\n","battle for power in \"Game of Thrones\" due to the insatiable appetite for compute power to run large\n","AI models.\n"]}],"source":["wrapped_text = textwrap.fill(output_summary['intermediate_steps'][2],\n","                             width=100,\n","                             break_long_words=False,\n","                             replace_whitespace=False)\n","print(wrapped_text)"]},{"cell_type":"markdown","metadata":{"id":"caaAmomfPv9j"},"source":["### With the 'refine' CombineDocument Chain"]},{"cell_type":"markdown","metadata":{"id":"39P6zjy9lT5X"},"source":["## Refine\n","This method involves **an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document**, asking the LLM to refine the output based on the new document.\n","\n","**Pros:** Can pull in more relevant context, and may be less lossy than MapReduceDocumentsChain.\n","\n","**Cons:** Requires many more calls to the LLM than StuffDocumentsChain. The calls are also NOT independent, meaning they cannot be paralleled like MapReduceDocumentsChain. There is also some potential dependencies on the ordering of the documents."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Ji2D8Q0P2gh"},"outputs":[],"source":["chain = load_summarize_chain(llm, chain_type=\"refine\")\n","\n","output_summary = chain.invoke(docs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGJ46apzKm3P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739281344867,"user_tz":-330,"elapsed":7,"user":{"displayName":"MANARANJAN PRADHAN","userId":"03885802779803335284"}},"outputId":"544b1358-3c04-43e1-bfe0-91cf8035776b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Access to Nvidia's high-performance H100 GPU is a highly sought-after and limited resource for\n","training large language models, with companies like Microsoft highlighting GPU availability as a\n","critical factor in their business operations. The demand for H100 GPUs is expected to continue until\n","at least the end of 2024, with various companies and cloud providers speculated to require large\n","quantities, including OpenAI, Inflection, Meta, and major cloud providers like Azure, Google Cloud,\n","and AWS, further exacerbating the limited availability of this resource. Estimates suggest that the\n","total demand could be around 432,000 H100 GPUs, valued at approximately $15 billion, although this\n","may involve double-counting and excludes additional demand from Chinese companies and financial\n","institutions, which are also expected to require significant quantities of H100s or similar GPUs,\n","such as the H800s or A100s. However, efforts to optimize machine learning models, such as those by\n","CentML, may help increase the efficiency of existing hardware, potentially reducing the need for\n","large volumes of high-end GPUs like the A100s, although this may have more impact on AI inference\n","rather than training large language models from scratch.\n"]}],"source":["wrapped_text = textwrap.fill(output_summary['output_text'], width=100)\n","print(wrapped_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VhFoMHoGKloi"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}